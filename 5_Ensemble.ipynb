{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed =  42\n",
      "libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Load usefull libraries and helper functions\n",
    "%run libs_and_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loaded\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv(r'input/creditcard_train.csv')\n",
    "df_train.sort_values(by=['Time'], inplace=True)\n",
    "df_test = pd.read_csv(r'input/creditcard_test.csv')\n",
    "df_test.sort_values(by=['Time'], inplace=True)\n",
    "\n",
    "X_train = df_train.drop(columns='Class')\n",
    "y_train = df_train.Class\n",
    "X_test = df_test.drop(columns='Class')\n",
    "y_test = df_test.Class\n",
    "print('data_loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ensemble the Models\n",
    "Simple averaging is used due to lack of time to do any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LGBMClassifier(class_weight='balanced', num_threads = -1, random_state=seed),\n",
    "    ExtraTreesClassifier(class_weight='balanced', n_jobs = -1, random_state=seed),\n",
    "    RandomForestClassifier(class_weight='balanced', n_jobs = -1, random_state=seed),\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier optimized parameters: {'num_leaves': 618, 'reg_lambda': 0.2008734705666495, 'reg_alpha': 0.24515892466388628, 'max_depth': 86, 'min_data_in_leaf': 381, 'max_bin': 1076, 'learning_rate': 0.43704503206629275}\n",
      "SMOTE_oversampling =  0.004180498819250385 ; SMOTE k_neighbors =  1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=381, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=381\n",
      "[LightGBM] [Warning] num_threads is set=-1, n_jobs=-1 will be ignored. Current value: num_threads=-1\n",
      "LGBMClassifier  auprc =  0.8358407450208815\n",
      "ExtraTreesClassifier optimized parameters: {'n_estimators': 649, 'max_depth': 26, 'criterion': 'entropy', 'min_samples_split': 10, 'max_features': 24}\n",
      "SMOTE_oversampling =  0.14941422526346582 ; SMOTE k_neighbors =  5\n",
      "ExtraTreesClassifier  auprc =  0.8320703151440892\n",
      "RandomForestClassifier optimized parameters: {'n_estimators': 131, 'max_depth': 22, 'min_samples_split': 4, 'max_features': 2}\n",
      "SMOTE_oversampling =  0.06865714527853939 ; SMOTE k_neighbors =  8\n",
      "RandomForestClassifier  auprc =  0.8227501769837371\n",
      "Bagging_ensemble_auprc =  0.8342756655266614\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_pred_probas=[]\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    # Load model and SMOTE best parameters\n",
    "    model, alpha_over, k_neighbors = load_best_parameters_sampling(model)\n",
    "    print('SMOTE_oversampling = ', alpha_over, '; SMOTE k_neighbors = ', k_neighbors)\n",
    "    # Resample train using SMOTE best parameters\n",
    "    over = SMOTE(random_state=42, sampling_strategy=alpha_over, k_neighbors=k_neighbors)\n",
    "    X_train_res, y_train_res = over.fit_resample(X_train, y_train)\n",
    "    #plot_auprc(model, X_train_res, y_train_res, X_test, y_test)\n",
    "    # Refit the model on whole train data\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    y_test_pred_proba=model.predict_proba(X_test)[:,1]\n",
    "    print(model_name, ' auprc = ', auprc(y_test, y_test_pred_proba))\n",
    "    y_test_pred_probas.append(y_test_pred_proba)    \n",
    "\n",
    "y_test_pred_proba_avg=np.mean(y_test_pred_probas, axis=0)\n",
    "print('Bagging_ensemble_auprc = ', auprc(y_test, y_test_pred_proba_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models AUPRC:\n",
    "1. **LGBM**         = **0.836**\n",
    "2. ExtraTrees   = 0.832\n",
    "3. RandomForest = 0.823\n",
    "- Simple bagging (averaging) ensemble = **0.834**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple NN architecture probably gives a much better score:\n",
    "https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
